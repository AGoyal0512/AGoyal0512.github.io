---
---

@inproceedings{goyal2023tensor,
  title={Tensor Decomposition to Capture Spatiotemporal Patterns of Coupled Oscillator and Opinion Dynamics},
  author={{<u>Agam Goyal</u>} and Lyu, Hanbaek},
  booktitle={Proceedings of the International Conference on Complex Networks and their Applications},
  volume={12},
  number={Book of Extended Abstracts},
  year={2023},
  abbr={CNA},
  code={https://github.com/AGoyal0512/NCPD-Dynamics},
  pdf={2023.ncpd_paper.pdf},
  preview={rep_figure.png},
  award={Oral Presentation}
}

@inproceedings{chuang2024simulating,
  title={Simulating Opinion Dynamics with Networks of LLM-based Agents},
  author={Chuang, Yun-Shiuan and {<u>Agam Goyal</u>} and Harlalka, Nikunj and Suresh, Siddharth and Hawkins, Robert and Yang, Sijia and Shah, Dhavan and Hu, Junjie and Rogers, Timothy T},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={3326--3346},
  year={2024},
  organization={2024 Association for Computational Linguistics},
  abbr={NAACL, ICLR'W},
  abstract={Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward: refining LLMs with real-world discourse to better simulate the evolution of human beliefs.},
  code={https://github.com/yunshiuan/llm-agent-opinion-dynamics},
  pdf={2024.findings-naacl.211.pdf},
  selected={true},
  preview={OD_figure.png}
}

@article{goyal2023latent,
  title={A latent linear model for nonlinear coupled oscillators on graphs},
  author={{<u>Agam Goyal</u>}* and Wu*, Zhaoxing and Yim, Richard P and Chen, Binhao and Xu, Zihong and Lyu, Hanbaek},
  journal={arXiv preprint arXiv:2311.14910},
  year={2023},
  abbr={arXiv},
  abstract={A system of coupled oscillators on an arbitrary graph is locally driven by the tendency to mutual synchronization between nearby oscillators, but can and often exhibit nonlinear behavior on the whole graph. Understanding such nonlinear behavior has been a key challenge in predicting whether all oscillators in such a system will eventually synchronize. In this paper, we demonstrate that, surprisingly, such nonlinear behavior of coupled oscillators can be effectively linearized in certain latent dynamic spaces. The key insight is that there is a small number of `latent dynamics filters', each with a specific association with synchronizing and non-synchronizing dynamics on subgraphs so that any observed dynamics on subgraphs can be approximated by a suitable linear combination of such elementary dynamic patterns. Taking an ensemble of subgraph-level predictions provides an interpretable predictor for whether the system on the whole graph reaches global synchronization. We propose algorithms based on supervised matrix factorization to learn such latent dynamics filters. We demonstrate that our method performs competitively in synchronization prediction tasks against baselines and black-box classification algorithms, despite its simple and interpretable architecture.},
  arxiv={2311.14910},
  code={https://github.com/Zhaoxing-Wu/Interpretable-ML-Sync},
  preview={lldm_scheme.png}
}

@inproceedings{chuang2024wisdom,
  title={The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents},
  author={Chuang, Yun-Shiuan and Harlalka*, Nikunj and Suresh*, Siddharth and {<u>Agam Goyal</u>} and Hawkins, Robert and Yang, Sijia and Shah, Dhavan and Hu, Junjie and Rogers, Timothy T},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={46},
  year={2024},
  abbr={CogSci, ICLR'W},
  abstract={Human groups are able to converge on more accurate beliefs through deliberation, even in the presence of polarization and partisan bias -- a phenomenon known as the "wisdom of partisan crowds." Generated agents powered by Large Language Models (LLMs) are increasingly used to simulate human collective behavior, yet few benchmarks exist for evaluating their dynamics against the behavior of human groups. In this paper, we examine the extent to which the wisdom of partisan crowds emerges in groups of LLM-based agents that are prompted to role-play as partisan personas (e.g., Democrat or Republican). We find that they not only display human-like partisan biases, but also converge to more accurate beliefs through deliberation as humans do. We then identify several factors that interfere with convergence, including the use of chain-of-thought prompt and lack of details in personas. Conversely, fine-tuning on human data appears to enhance convergence. These findings show the potential and limitations of LLM-based agents as a model of human collective intelligence.},
  code={https://github.com/Knowledge-and-Concepts-Lab/llm-becker-2019},
  pdf={2024_cogsci_paper.pdf},
  preview={woc.jpg}
}

@inproceedings{misra2024uncovering,
  title={Uncovering the Hidden Cost of Model Compression},
  author={Misra*, Diganta and Chaudhary*, Muawiz and {<u>Agam Goyal</u>}* and Runwal*, Bharat and Chen, Pin Yu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1611--1621},
  year={2024},
  abbr={CVPR},
  abstract={In an age dominated by resource-intensive foundation models, the ability to efficiently adapt to downstream tasks is crucial. Visual Prompting (VP), drawing inspiration from the prompting techniques employed in Large Language Models (LLMs), has emerged as a pivotal method for transfer learning in the realm of computer vision. As the importance of efficiency continues to rise, research into model compression has become indispensable in alleviating the computational burdens associated with training and deploying over-parameterized neural networks. A primary objective in model compression is to develop sparse and/or quantized models capable of matching or even surpassing the performance of their over-parameterized, full-precision counterparts. Although previous studies have explored the effects of model compression on transfer learning, its impact on visual prompting-based transfer remains unclear. This study aims to bridge this gap, shedding light on the fact that model compression detrimentally impacts the performance of visual prompting-based transfer, particularly evident in scenarios with low data volume. Furthermore, our findings underscore the adverse influence of sparsity on the calibration of downstream visual-prompted models. However, intriguingly, we also illustrate that such negative effects on calibration are not present when models are compressed via quantization. This empirical investigation underscores the need for a nuanced understanding beyond mere accuracy in sparse and quantized settings, thereby paving the way for further exploration in Visual Prompting techniques tailored for sparse and quantized models.},
  code={https://github.com/landskape-ai/Reprogram_LT},
  pdf={2024_CVPRW_paper.pdf},
  supp={2024_CVPRW_supp.pdf},
  preview={vp.png}
}

@inproceedings{chuang-etal-2024-beyond,
    title = "Beyond Demographics: Aligning Role-playing {LLM}-based Agents Using Human Belief Networks",
    author = "Chuang, Yun-Shiuan  and
      Nirunwiroj, Krirk  and
      Studdiford, Zach  and
      Goyal, Agam  and
      Frigo, Vincent V.  and
      Yang, Sijia  and
      Shah, Dhavan V.  and
      Hu, Junjie  and
      Rogers, Timothy T.",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.819",
    pages = "14010--14026",
    abstract = "Creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Using data from a human survey, we estimated a belief network encompassing 64 topics loading on nine non-overlapping latent factors. We then seeded LLM-based agents with an opinion on one topic, and assessed the alignment of its expressed opinions on remaining test topics with corresponding human data. Role-playing based on demographic information alone did not align LLM and human opinions, but seeding the agent with a single belief greatly improved alignment for topics related in the belief network, and not for topics outside the network. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society.",
    pdf={https://aclanthology.org/2024.findings-emnlp.819.pdf},
    selected={true},
    preview={benign_pic.jpg},
    abbr={EMNLP, NeurIPS'W},
}

@inproceedings{zhan-etal-2025-slm,
    title = "SLM-Mod: Small Language Models Surpass LLMs at Content Moderation",
    author = "Zhan, Xianyang  and
      Goyal, Agam  and
      Chen, Yilun  and
      Chandrasekharan, Eshwar  and
      Saha, Koustuv",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.441/",
    pages = "8774--8790",
    ISBN = "979-8-89176-189-6",
    abstract = "Large language models (LLMs) have shown promise in many natural language understanding tasks, including content moderation. However, these models can be expensive to query in real-time and do not allow for a community-specific approach to content moderation. To address these challenges, we explore the use of open-source small language models (SLMs) for community-specific content moderation tasks. We fine-tune and evaluate SLMs (less than 15B parameters) by comparing their performance against much larger open- and closed-sourced models in both a zero-shot and few-shot setting. Using 150K comments from 15 popular Reddit communities, we find that SLMs outperform zero-shot LLMs at content moderation$-11.5${\%} higher accuracy and 25.7{\%} higher recall on average across all communities. Moreover, few-shot in-context learning leads to only a marginal increase in the performance of LLMs, still lacking compared to SLMs. We further show the promise of cross-community content moderation, which has implications for new communities and the development of cross-platform moderation techniques. Finally, we outline directions for future work on language model based content moderation.",
    abbr={NAACL},
    selected={true},
    pdf={https://aclanthology.org/2025.naacl-long.441.pdf},
    preview={slm_mod.png}
}

@article{goyal2024uncovering,
  title={Uncovering the Internet's Hidden Values: An Empirical Study of Desirable Behavior Using Highly-Upvoted Content on Reddit},
  author={{<u>Agam Goyal</u>} and Lambert, Charlotte and Jain, Yoshee and Chandrasekharan, Eshwar},
  journal={arXiv preprint arXiv:2410.13036},
  year={2024},
  abbr={arXiv},
  abstract={A major task for moderators of online spaces is norm-setting, essentially creating shared norms for user behavior in their communities. Platform design principles emphasize the importance of highlighting norm-adhering examples and explicitly stating community norms. However, norms and values vary between communities and go beyond content-level attributes, making it challenging for platforms and researchers to provide automated ways to identify desirable behavior to be highlighted. Current automated approaches to detect desirability are limited to measures of prosocial behavior, but we do not know whether these measures fully capture the spectrum of what communities value. In this paper, we use upvotes, which express community approval, as a proxy for desirability and examine 16,000 highly-upvoted comments across 80 popular sub-communities on Reddit. Using a large language model, we extract values from these comments across two years (2016 and 2022) and compile 64 and 72 macro, meso, and micro values for 2016 and 2022 respectively, based on their frequency across communities. Furthermore, we find that existing computational models for measuring prosociality were inadequate to capture on average 82% of the values we extracted. Finally, we show that our approach can not only extract most of the qualitatively-identified values from prior taxonomies, but also uncover new values that are actually encouraged in practice. Our findings highlight the need for nuanced models of desirability that go beyond preexisting prosocial measures. This work has implications for improving moderator understanding of their community values and provides a framework that can supplement qualitative approaches with larger-scale content analyses.},
  arxiv={2410.13036},
  preview={upvote_teaser.png}
}
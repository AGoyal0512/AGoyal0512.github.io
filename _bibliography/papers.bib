---
---

@inproceedings{goyal2023tensor,
  title={Tensor Decomposition to Capture Spatiotemporal Patterns of Coupled Oscillator and Opinion Dynamics},
  author={{<u>Agam Goyal</u>} and Lyu, Hanbaek},
  booktitle={Proceedings of the International Conference on Complex Networks and their Applications},
  volume={12},
  number={Book of Extended Abstracts},
  year={2023},
  abbr={CNA},
  code={https://github.com/AGoyal0512/NCPD-Dynamics},
  pdf={2023.ncpd_paper.pdf},
  preview={rep_figure.png},
  award={Oral Presentation}
}

@inproceedings{chuang2024simulating,
  title={Simulating Opinion Dynamics with Networks of LLM-based Agents},
  author={Chuang, Yun-Shiuan and {<u>Agam Goyal</u>} and Harlalka, Nikunj and Suresh, Siddharth and Hawkins, Robert and Yang, Sijia and Shah, Dhavan and Hu, Junjie and Rogers, Timothy T},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={3326--3346},
  year={2024},
  organization={2024 Association for Computational Linguistics},
  abbr={NAACL, ICLR},
  abstract={Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward: refining LLMs with real-world discourse to better simulate the evolution of human beliefs.},
  code={https://github.com/yunshiuan/llm-agent-opinion-dynamics},
  pdf={2024.findings-naacl.211.pdf},
  selected={true},
  preview={OD_figure.png}
}

@article{goyal2023latent,
  title={A latent linear model for nonlinear coupled oscillators on graphs},
  author={{<u>Agam Goyal</u>}* and Wu*, Zhaoxing and Yim, Richard P and Chen, Binhao and Xu, Zihong and Lyu, Hanbaek},
  journal={arXiv preprint arXiv:2311.14910},
  year={2023},
  abbr={arXiv},
  abstract={A system of coupled oscillators on an arbitrary graph is locally driven by the tendency to mutual synchronization between nearby oscillators, but can and often exhibit nonlinear behavior on the whole graph. Understanding such nonlinear behavior has been a key challenge in predicting whether all oscillators in such a system will eventually synchronize. In this paper, we demonstrate that, surprisingly, such nonlinear behavior of coupled oscillators can be effectively linearized in certain latent dynamic spaces. The key insight is that there is a small number of `latent dynamics filters', each with a specific association with synchronizing and non-synchronizing dynamics on subgraphs so that any observed dynamics on subgraphs can be approximated by a suitable linear combination of such elementary dynamic patterns. Taking an ensemble of subgraph-level predictions provides an interpretable predictor for whether the system on the whole graph reaches global synchronization. We propose algorithms based on supervised matrix factorization to learn such latent dynamics filters. We demonstrate that our method performs competitively in synchronization prediction tasks against baselines and black-box classification algorithms, despite its simple and interpretable architecture.},
  arxiv={2311.14910},
  code={https://github.com/Zhaoxing-Wu/Interpretable-ML-Sync},
  selected={true},
  preview={lldm_scheme.png}
}

@inproceedings{chuang2024wisdom,
  title={The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents},
  author={Chuang, Yun-Shiuan and Harlalka*, Nikunj and Suresh*, Siddharth and {<u>Agam Goyal</u>} and Hawkins, Robert and Yang, Sijia and Shah, Dhavan and Hu, Junjie and Rogers, Timothy T},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={46},
  year={2024},
  abbr={CogSci, ICLR},
  abstract={Human groups are able to converge on more accurate beliefs through deliberation, even in the presence of polarization and partisan bias -- a phenomenon known as the "wisdom of partisan crowds." Generated agents powered by Large Language Models (LLMs) are increasingly used to simulate human collective behavior, yet few benchmarks exist for evaluating their dynamics against the behavior of human groups. In this paper, we examine the extent to which the wisdom of partisan crowds emerges in groups of LLM-based agents that are prompted to role-play as partisan personas (e.g., Democrat or Republican). We find that they not only display human-like partisan biases, but also converge to more accurate beliefs through deliberation as humans do. We then identify several factors that interfere with convergence, including the use of chain-of-thought prompt and lack of details in personas. Conversely, fine-tuning on human data appears to enhance convergence. These findings show the potential and limitations of LLM-based agents as a model of human collective intelligence.},
  code={https://github.com/Knowledge-and-Concepts-Lab/llm-becker-2019},
  pdf={2024_cogsci_paper.pdf},
  preview={woc.jpg}
}

@inproceedings{misra2024uncovering,
  title={Uncovering the Hidden Cost of Model Compression},
  author={Misra*, Diganta and Chaudhary*, Muawiz and {<u>Agam Goyal</u>}* and Runwal*, Bharat and Chen, Pin Yu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1611--1621},
  year={2024},
  abbr={CVPR},
  abstract={In an age dominated by resource-intensive foundation models, the ability to efficiently adapt to downstream tasks is crucial. Visual Prompting (VP), drawing inspiration from the prompting techniques employed in Large Language Models (LLMs), has emerged as a pivotal method for transfer learning in the realm of computer vision. As the importance of efficiency continues to rise, research into model compression has become indispensable in alleviating the computational burdens associated with training and deploying over-parameterized neural networks. A primary objective in model compression is to develop sparse and/or quantized models capable of matching or even surpassing the performance of their over-parameterized, full-precision counterparts. Although previous studies have explored the effects of model compression on transfer learning, its impact on visual prompting-based transfer remains unclear. This study aims to bridge this gap, shedding light on the fact that model compression detrimentally impacts the performance of visual prompting-based transfer, particularly evident in scenarios with low data volume. Furthermore, our findings underscore the adverse influence of sparsity on the calibration of downstream visual-prompted models. However, intriguingly, we also illustrate that such negative effects on calibration are not present when models are compressed via quantization. This empirical investigation underscores the need for a nuanced understanding beyond mere accuracy in sparse and quantized settings, thereby paving the way for further exploration in Visual Prompting techniques tailored for sparse and quantized models.},
  code={https://github.com/landskape-ai/Reprogram_LT},
  pdf={2024_CVPRW_paper.pdf},
  supp={2024_CVPRW_supp.pdf},
  selected={true},
  preview={vp.png}
}

@article{chuang2024beyond,
  title={Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks},
  author={Chuang, Yun-Shiuan and Studdiford*, Zach and Nirunwiroj*, Krirk and {<u>Agam Goyal</u>} and Frigo, Vincent V and Yang, Sijia and Shah, Dhavan and Hu, Junjie and Rogers, Timothy T},
  booktitle={Findings of the Empirical Methods in Natural Language Processing: EMNLP 2024},
  pages={9999},
  year={2024},
  organization={2024 Association for Computational Linguistics},
  abbr={EMNLP},
  abstract={Creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Using data from a human survey, we estimated a belief network encompassing 18 topics loading on two non-overlapping latent factors. We then seeded LLM-based agents with an opinion on one topic, and assessed the alignment of its expressed opinions on remaining test topics with corresponding human data. Role-playing based on demographic information alone did not align LLM and human opinions, but seeding the agent with a single belief greatly improved alignment for topics related in the belief network, and not for topics outside the network. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society.},
  arxiv={2406.17232},
  selected={true},
  preview={benign_pic.jpg}
}
